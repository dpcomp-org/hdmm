{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from hdmm.templates import TemplateStrategy\n",
    "from hdmm import workload, matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class McKennaConvex(TemplateStrategy):\n",
    "    \"\"\"\n",
    "    A highly optimized GPU implementation of OPT_0.  \n",
    "    Requires memory for 6*n^2 4 byte floats\n",
    "    \n",
    "    For n = 2^14 = 16384, requires ~25 seconds per iteration\n",
    "    \"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        #self._mask = torch.tril(torch.ones(n,n, dtype=torch.uint8, device=\"cuda\"), diagonal=-1)\n",
    "        #self._params = torch.zeros(n*(n-1)//2, device=\"cuda\")\n",
    "        #self.X = torch.eye(n,n, device=\"cuda\")\n",
    "        self.iX = torch.empty(n,n,device=\"cuda\")\n",
    "        self.tmp = torch.empty(n,n,device=\"cuda\")\n",
    "        self.grad = torch.empty(n,n,device=\"cuda\")\n",
    "        \n",
    "    def strategy(self):\n",
    "        torch.cholesky(self.X, out=self.tmp)\n",
    "        A = self.tmp.to(\"cpu\").numpy()\n",
    "        return matrix.EkteloMatrix(A.T)\n",
    "\n",
    "    def _set_workload(self, W):\n",
    "        self.V = torch.tensor(W.gram().dense_matrix()).to(\"cuda\")\n",
    "        self.W = W\n",
    "        \n",
    "    def _loss(self, X):\n",
    "        try:\n",
    "            torch.cholesky(X, out=self.tmp)\n",
    "            torch.cholesky_inverse(self.tmp, out=self.iX)\n",
    "            #self.iX = torch.inverse(X, out=self.iX)\n",
    "        except:\n",
    "            return torch.tensor(np.inf, device=\"cuda\")\n",
    "      \n",
    "        return torch.dot(self.iX.flatten(), self.V.flatten())\n",
    "\n",
    "    def _ngrad(self):\n",
    "        # negative gradient, should be called immediately after _loss\n",
    "        self.tmp = torch.mm(self.iX, self.V, out=self.tmp)\n",
    "        self.grad = torch.mm(self.tmp, self.iX, out=self.grad)\n",
    "        #G = -self.iX @ self.V @ self.iX\n",
    "        torch.zeros(self.n, out=self.grad.diagonal())\n",
    "        return self.grad\n",
    "\n",
    "    def optimize(self, W, iters=5000):\n",
    "        self._set_workload(W)\n",
    "\n",
    "        eig, P = torch.symeig(self.V, eigenvectors=True)\n",
    "        eig[eig < 1e-10] = 0.0\n",
    "        X = P @ torch.diag(torch.sqrt(eig)) @ torch.t(P)\n",
    "        X /= torch.diag(X).max()\n",
    "        torch.ones(self.n, out=X.diagonal())\n",
    "\n",
    "        Y = torch.empty(self.n, self.n, device=\"cuda\")\n",
    "        \n",
    "        # have to implement the optimization loop manually :(\n",
    "        \n",
    "        loss= self._loss(X)\n",
    "        \n",
    "        beta = 0.25\n",
    "        for it in range(250):\n",
    "            beta *= 4.0\n",
    "            t0 = time.time()\n",
    "            curr_loss = loss\n",
    "            ngrad = self._ngrad().mul_(beta)\n",
    "            m = ngrad.norm()\n",
    "            t1 = time.time()\n",
    "            Y.copy_(X)\n",
    "            for i in range(0, 25):\n",
    "                torch.add(Y, ngrad, out=X)\n",
    "                loss = self._loss(X)\n",
    "                if curr_loss - loss >= 0.5*beta*m:\n",
    "                    break\n",
    "                beta *= 0.5\n",
    "                ngrad.mul_(0.5)\n",
    "            t2 = time.time()\n",
    "            if it % 1 == 0:\n",
    "                print('%d, %.2f, %.2f, %.6f, %.4f' % (it, t1-t0, t2-t1, beta,torch.sqrt(loss/self.W.shape[0])))\n",
    "            \n",
    "        self.X = X\n",
    "            \n",
    "        return loss\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0.00, 24.75, 0.015625, 4.4598\n",
      "1, 0.00, 25.52, 0.000122, 4.4593\n",
      "2, 0.00, 23.69, 0.000488, 4.4574\n",
      "3, 0.00, 23.82, 0.001953, 4.4502\n",
      "4, 0.00, 24.05, 0.001953, 4.4433\n",
      "5, 0.00, 24.21, 0.001953, 4.4367\n",
      "6, 0.00, 23.98, 0.003906, 4.4242\n",
      "7, 0.00, 23.88, 0.007812, 4.4016\n",
      "8, 0.00, 25.13, 0.000122, 4.4010\n",
      "9, 0.00, 23.87, 0.000488, 4.3998\n"
     ]
    }
   ],
   "source": [
    "n = 2**14\n",
    "W = workload.Prefix(n, dtype=np.float32)\n",
    "#W = workload.AllRange(n)\n",
    "temp = McKennaConvex(n)\n",
    "temp.optimize(W)\n",
    "\n",
    "A = temp.strategy().dense_matrix()\n",
    "np.save('prefix-%d.npy' % n, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
